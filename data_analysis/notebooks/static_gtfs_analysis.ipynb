{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ff509f7",
   "metadata": {},
   "source": [
    "# Notebook with GTFS methods\n",
    "\n",
    "Goals: \n",
    "\n",
    "* Make a way to calculate the scheduled number of current active trips given a date, time, and route. \n",
    "    - Take datetime and find what services are active on that date \n",
    "    - Find what trips run on those services + route \n",
    "    - Find which of those trips are \"in progress\" per stop_times\n",
    "* ~Output most common shape by route~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffac7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "\n",
    "import boto3\n",
    "import os\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import requests\n",
    "import pendulum\n",
    "from io import BytesIO\n",
    "import shapely\n",
    "import geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a22d9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if \"private\", will assume you have write permissions and allow you to write; else will not attempt to write files\n",
    "BUCKET_TYPE = \"private\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41f5df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local \n",
    "# CTA_GTFS = zipfile.ZipFile('cta_gtfs_20220509.zip')\n",
    "# s3\n",
    "# follow https://pythonguides.com/download-zip-file-from-url-using-python/\n",
    "# CTA_GTFS = zipfile.ZipFile(BytesIO(requests.get('https://chn-ghost-buses-public.s3.us-east-2.amazonaws.com/cta_static_gtfs/cta_gtfs_20220509.zip').content))\n",
    "# cta website\n",
    "\n",
    "# VERSION_ID = '20220718'\n",
    "\n",
    "RTD_GTFS = zipfile.ZipFile('../../utils/utils/gtfs/google_transit_20231504.zip') # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60357a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTFSFeed:\n",
    "   \"\"\" Static GTFS management \"\"\"\n",
    "   def __init__(self, gtfs_zipfile):\n",
    "        self.gtfs_zipfile = gtfs_zipfile\n",
    "        self.feed_start = None\n",
    "        self.feed_end = None\n",
    "        try: \n",
    "            with self.gtfs_zipfile.open('stops.txt') as file:\n",
    "                    self.stops = pd.read_csv(file, dtype = 'object')\n",
    "                    print(\"stops.txt loaded\")\n",
    "            with self.gtfs_zipfile.open('stop_times.txt') as file:\n",
    "                    self.stop_times = pd.read_csv(file, dtype = 'object')\n",
    "                    print(\"stop_times.txt loaded\")\n",
    "            with self.gtfs_zipfile.open('routes.txt') as file:\n",
    "                    self.routes = pd.read_csv(file, dtype = 'object')\n",
    "                    print(\"routes.txt loaded\")\n",
    "            with self.gtfs_zipfile.open('trips.txt') as file:\n",
    "                    self.trips = pd.read_csv(file, dtype = 'object')\n",
    "                    print(\"trips.txt loaded\")\n",
    "        except KeyError as e:\n",
    "            print(\"GTFS is missing required file\")\n",
    "            print(e)\n",
    "        if 'calendar.txt' in self.gtfs_zipfile.namelist():\n",
    "                with self.gtfs_zipfile.open('calendar.txt') as file:\n",
    "                        self.calendar = pd.read_csv(file, dtype = 'object')\n",
    "                        print(\"calendar.txt loaded\")\n",
    "        else:\n",
    "            print(\"no calendar.txt found\")\n",
    "        if 'calendar_dates.txt' in self.gtfs_zipfile.namelist():\n",
    "                with self.gtfs_zipfile.open('calendar_dates.txt') as file:\n",
    "                        self.calendar_dates = pd.read_csv(file, dtype = 'object')\n",
    "                        print(\"calendar_dates.txt loaded\")\n",
    "        else:\n",
    "            print(\"no calendar_dates.txt found\")\n",
    "        if 'shapes.txt' in self.gtfs_zipfile.namelist():\n",
    "                with self.gtfs_zipfile.open('shapes.txt') as file:\n",
    "                        self.shapes = pd.read_csv(file, dtype = 'object')\n",
    "                        print(\"shapes.txt loaded\")\n",
    "        else:\n",
    "            print(\"no shapes.txt found\")\n",
    "        if 'feed_info.txt' in self.gtfs_zipfile.namelist():\n",
    "                with self.gtfs_zipfile.open('feed_info.txt') as file:\n",
    "                        self.feed_info = pd.read_csv(file, dtype = 'object')\n",
    "                        print(\"feed_info.txt loaded\")\n",
    "                self.feed_start = pd.to_datetime(self.feed_info['feed_start_date'][0])\n",
    "                self.feed_end = pd.to_datetime(self.feed_info['feed_end_date'][0])\n",
    "        else:\n",
    "            print(\"no feed_info.txt found\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a4870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = GTFSFeed(RTD_GTFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e879fed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.feed_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf546a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Convert calen\n",
    "data.calendar\n",
    "# data.calendar_dates\n",
    "# data.trips\n",
    "# data.stop_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740f84c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hour(s):\n",
    "    parts = s.split(':')\n",
    "    assert len(parts)==3\n",
    "    hour = int(parts[0])\n",
    "    if hour >= 24:\n",
    "        hour -= 24\n",
    "    return hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22148f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data.calendar['start_date_dt'] = pd.to_datetime(data.calendar['start_date'], format='%Y%m%d' ).dt.tz_localize('America/Denver')\n",
    "data.calendar['end_date_dt'] = pd.to_datetime(data.calendar['end_date'], format='%Y%m%d' ).dt.tz_localize('America/Denver')\n",
    "data.calendar_dates['date_dt'] = pd.to_datetime(data.calendar_dates['date'], format='%Y%m%d' ).dt.tz_localize('America/Denver')\n",
    "\n",
    "# extract hour from stop_times timestamps \n",
    "data.stop_times['arrival_hour'] = data.stop_times.arrival_time.apply(lambda x: get_hour(x))\n",
    "data.stop_times['departure_hour'] = data.stop_times.departure_time.apply(lambda x: get_hour(x))\n",
    "data.calendar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9217f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def format_dates_hours(data):\n",
    "#     # convert string dates to actual datetimes in calendar.txt and calendar_dates.txt\n",
    "#     data.calendar['start_date_dt'] = data.calendar['start_date'].apply(lambda x: pendulum.from_format(x, 'YYYYMMDD', tz='America/Denver'))\n",
    "#     data.calendar['end_date_dt'] = data.calendar['end_date'].apply(lambda x: pendulum.from_format(x, 'YYYYMMDD', tz='America/Denver'))\n",
    "#     data.calendar_dates['date_dt'] = data.calendar_dates['date'].apply(lambda x: pendulum.from_format(x, 'YYYYMMDD', tz='America/Denver'))\n",
    "    \n",
    "#     # extract hour from stop_times timestamps \n",
    "#     data.stop_times['arrival_hour'] = data.stop_times.arrival_time.apply(lambda x: get_hour(x))\n",
    "#     data.stop_times['departure_hour'] = data.stop_times.departure_time.apply(lambda x: get_hour(x))\n",
    "    \n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43200f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = format_dates_hours(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc1da23",
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_date_range = pd.DataFrame(pd.date_range(data.feed_start, data.feed_end, tz = 'America/Denver'), columns = ['raw_date'])\n",
    "    \n",
    "#     # cross join calendar index with actual calendar to get all combos of possible dates & services \n",
    "calendar_cross = calendar_date_range.merge(data.calendar, how = \"cross\")\n",
    "\n",
    "#     # extract day of week from date index date\n",
    "calendar_cross['dayofweek'] = calendar_cross['raw_date'].dt.dayofweek\n",
    "calendar_cross\n",
    "\n",
    "    # take wide calendar data (one col per day of week) and make it long (one row per day of week)\n",
    "scheduled_service = calendar_cross.melt(id_vars = ['raw_date', 'start_date', 'end_date', 'service_id', 'dayofweek'], var_name = 'cal_dayofweek', value_name = 'cal_val')\n",
    "\n",
    "# #     # map the calendar input strings to day of week integers to align w pandas dayofweek output\n",
    "scheduled_service['cal_daynum'] = scheduled_service['cal_dayofweek'].map({\n",
    "    'monday': 0,\n",
    "    'tuesday': 1,\n",
    "    'wednesday': 2,\n",
    "    'thursday': 3,\n",
    "    'friday': 4,\n",
    "    'saturday': 5,\n",
    "    'sunday': 6\n",
    "})\n",
    "scheduled_service = scheduled_service[(scheduled_service.dayofweek == scheduled_service.cal_daynum) & \n",
    "                                   (scheduled_service.start_date <= scheduled_service.raw_date) &\n",
    "                                   (scheduled_service.end_date >= scheduled_service.raw_date)]\n",
    "    \n",
    "# now merge in calendar dates to the datetime index to get overrides\n",
    "scheduled_service = scheduled_service.merge(data.calendar_dates, how = 'outer', left_on = ['raw_date', 'service_id'], right_on = ['date_dt', 'service_id'])\n",
    "\n",
    "# # now add a service happened flag for dates where the schedule indicates that this service occurred\n",
    "# # i.e.: calendar has a service indicator of 1 and there's no exception type from calendar_dates\n",
    "# # OR calendar_dates has exception type of 1\n",
    "# # otherwise no service \n",
    "# # https://stackoverflow.com/questions/21415661/logical-operators-for-boolean-indexing-in-pandas\n",
    "scheduled_service['scheduled_service_flag'] = ((scheduled_service['cal_val'] == '1') & \n",
    "                                        scheduled_service['exception_type'].isnull()) | (scheduled_service['exception_type'] == '1')\n",
    "# Note: Really a \"service_scheduled\", not \"service_happened\"\n",
    "\n",
    "\n",
    "\n",
    "# # now fill in rows where calendar_dates had a date outside the bounds of the datetime index, so raw_date is always populated\n",
    "scheduled_service['raw_date'] = scheduled_service['raw_date'].fillna(scheduled_service['date_dt'])\n",
    "\n",
    "# # filter to only rows where service occurred\n",
    "scheduled_service = scheduled_service[scheduled_service.scheduled_service_flag]\n",
    "\n",
    "# # join trips to only service that occurred\n",
    "trips_scheduled = data.trips.merge(scheduled_service, how = 'left', on = 'service_id')\n",
    "\n",
    "\n",
    "# \n",
    "# # get only the trip / hour combos that actually occurred\n",
    "# Drop this, since it's assuming one-stop-per-trip\n",
    "# trip_stop_hours = data.stop_times[['trip_id', 'arrival_hour']].drop_duplicates()\n",
    "\n",
    "# # now join\n",
    "# # result has one row per date + row from trips.txt (incl. route) + hour\n",
    "# trip_summary = trips_happened.merge(trip_stop_hours, how = \"left\", on = \"trip_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d967da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stops - reduce fields to not-derivable \n",
    "# stops_scheduled = \n",
    "trips_scheduled.head()\n",
    "trips_scheduled_keep_cols = ['block_id',\t'route_id',\t'direction_id',\t'trip_headsign',\t'shape_id',\t'service_id',\t'trip_id',\t'raw_date']\n",
    "stop_time_keep_cols = ['trip_id',\t'arrival_time',\t'departure_time',\t'stop_id',\t'stop_sequence',\t'stop_headsign',\t'pickup_type',\t'drop_off_type',\t'shape_dist_traveled',\t'timepoint'] \n",
    "# Note that 'shape_dist_traveled',\t'timepoint' are not used for RTD, but could be useful elsewhere\n",
    "data.stop_times\n",
    "stops_scheduled = trips_scheduled[trips_scheduled_keep_cols].merge(data.stop_times[stop_time_keep_cols], on = 'trip_id' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b889d54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8db4413",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.stop_times.astype({'stop_sequence': 'int32'}).groupby('trip_id')['stop_sequence'].agg('max').median() #.plot(kind='hist') # value_counts() #plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d5c5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983ca911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform trips + stops into easily-worked-with final datasets\n",
    "\n",
    "# Trips:\n",
    "# * rename raw_date to scheduled_service_date\n",
    "# trips_scheduled = trips_scheduled[trips_scheduled_keep_cols]\n",
    "# trips_scheduled_rename_dict = {'raw_date':'scheduled_service_date'}\n",
    "# trips_scheduled = trips_scheduled.rename(trips_scheduled_rename_dict, axis = 1)\n",
    "\n",
    "\n",
    "# Stops:\n",
    "# * Convert arrival_time and departure_time into proper datetimes. Note: Slow!\n",
    "# stops_scheduled.head()\n",
    "def combine_day_stop(raw_dt, stop_time):\n",
    "    \"\"\" Only treat as string unless not possible elsewhere - to_datetime is incredibly slow in apply \"\"\"\n",
    "    try:\n",
    "        if int(stop_time[0:2]) > 23:\n",
    "            # Replace hour with (hour-24). Add one day to raw_dt.\n",
    "            hour_replace = str(int(stop_time[0:2]) - 24).zfill(2)\n",
    "            stop_time = hour_replace + stop_time[2:]\n",
    "            arrival_time = pd.to_datetime(f\"{raw_dt.date() + pd.Timedelta('1d')} {stop_time}\").tz_localize(raw_dt.tz)\n",
    "        else:\n",
    "            arrival_time = pd.to_datetime(f\"{raw_dt.date()} {stop_time}\").tz_localize(raw_dt.tz) # f\"{raw_dt.date()} {stop_time}\" \n",
    "        return arrival_time\n",
    "    except:\n",
    "        return None\n",
    "# Parallel-process \"apply\". Can't easily be vectorized.\n",
    "# stops_scheduled['arrival_datetime'] = stops_scheduled.swifter.apply(lambda x: combine_day_stop(x.raw_date, x.arrival_time), axis = 1)\n",
    "# stops_scheduled['departure_datetime'] = stops_scheduled.swifter.apply(lambda x: combine_day_stop(x.raw_date, x.departure_time), axis = 1)\n",
    "\n",
    "# # stops_scheduled['arrival_datetime'] = stops_scheduled.apply(lambda x: combine_day_stop(x.raw_date, x.arrival_time), axis = 1)\n",
    "# # stops_scheduled['departure_datetime'] = stops_scheduled.apply(lambda x: combine_day_stop(x.raw_date, x.departure_time), axis = 1)\n",
    "# stops_scheduled['arrival_datetime'] = pd.to_datetime(stops_scheduled['arrival_datetime']).tz_localize('America/Denver')\n",
    "# stops_scheduled['departure_datetime'] = pd.to_datetime(stops_scheduled['departure_datetime']).tz_localize('America/Denver')\n",
    "\n",
    "# * Rename \"raw_date\" to \"service_date\"\n",
    "# stops_scheduled_rename_dict = {'raw_date':'service_date'}\n",
    "# stops_scheduled = stops_scheduled.rename(stops_scheduled_rename_dict, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52e5c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f72457",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops_scheduled_ddf = dd.from_pandas(stops_scheduled)\n",
    "arrival_datetime = stops_scheduled_ddf.apply(lambda x: combine_day_stop(x.raw_date, x.arrival_time), axis = 1)\n",
    "departure_datetime = stops_scheduled_ddf.apply(lambda x: combine_day_stop(x.raw_date, x.departure_time), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b870c6f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e563dc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops_scheduled['arrival_datetime'].value_counts.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfec21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "# stops_scheduled - dataframe with all possible scheduled stops - ['trip_id',\t'arrival_time',\t'departure_time',\t'stop_id',\t'stop_sequence',\t'stop_headsign',\t'pickup_type',\t'drop_off_type',\t'shape_dist_traveled',\t'timepoint'] \n",
    "# trips_scheduled - dataframe with all possible scheduled trips - ['block_id',\t'route_id',\t'direction_id',\t'trip_headsign',\t'shape_id',\t'service_id',\t'trip_id',\t'raw_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9340c654",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops_scheduled.shape # 59 million scheduled stops!\n",
    "# How many before Memorial Day? 37.7 million.\n",
    "len(stops_scheduled.loc[stops_scheduled['raw_date'] < pd.to_datetime('2023-05-29 00:00:00-06:00')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fdc16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity-checks:\n",
    "trips_scheduled\n",
    "# raw_date + trip_id + service_id\n",
    "trips_scheduled.sort_values(['raw_date', 'trip_id', 'service_id'])\n",
    "# 1.5 million trips scheduled between Jan - May. Sanity-check:\n",
    "trip_days = 160 # approximate\n",
    "unique_routes = len(trips_scheduled['route_id'].unique()) *2 # Double for direction\n",
    "len(trips_scheduled) / trip_days / unique_routes # ~40ish round-trips per route per day. High? Reasonable?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a8afd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What other sanity-checks? Plot time series of trips / day\n",
    "trips_scheduled.sort_values(['raw_date', 'trip_id', 'service_id']).groupby('raw_date')['trip_id'].agg('count').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25d1ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many unique routes / day? 115ish weekday, 90ish weekend\n",
    "trips_scheduled.sort_values(['raw_date', 'route_id', 'service_id']).groupby('raw_date')['route_id'].agg('nunique').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efe336f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual inspection to double-check - Look at the 0\n",
    "trips_scheduled.sort_values(['raw_date', 'route_id', 'trip_id', 'service_id']).loc[(trips_scheduled.route_id == '0') & (trips_scheduled.raw_date == '2023-01-08 00:00:00-07:00')] #.head()\n",
    "# 123 rows. Double-checked against Transit, sanity-check passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183650c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.trips.loc[data.trips.trip_id.isin(['114346478','114346479','114346480','114346481'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992c5506",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.trips.loc[data.trips.trip_id.isin(['114346478','114346479','114346480','114346481'])]\n",
    "print(data.stop_times[data.stop_times.trip_id.isin(['114346478','114346479'])][['trip_id', 'departure_time']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346c4e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc163c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b3bb9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9913b3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba1950a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.calendar_dates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2c84c30",
   "metadata": {},
   "source": [
    "## Basic data transformations\n",
    "\n",
    "Ex. creating actual timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80c1328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_timestamp(s, date):\n",
    "#     parts = s.split(':')\n",
    "#     assert len(parts)==3\n",
    "#     if int(parts[0]) > 23:\n",
    "#         num_parts = [int(parts[0]) - 24, int(parts[1]), int(parts[2])]\n",
    "#     else:\n",
    "#         num_parts = [int(parts[0]), int(parts[1]), int(parts[2])]\n",
    "#     return pendulum.datetime(year = date.year, month = date.month, day = date.day, hour = num_parts[0], minute = num_parts[1], second = num_parts[2])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96410a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that there are no dwell periods that cross hour boundary\n",
    "# 476 rows - arrive at 59, leave a minute or two later. 476 instances.\n",
    "data.stop_times[data.stop_times.arrival_hour != data.stop_times.departure_hour]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7222f33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ad4649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_trip_summary(data):\n",
    "    # construct a datetime index that has every day between calendar start and end \n",
    "    calendar_date_range = pd.DataFrame(pd.date_range(min(data.calendar.start_date_dt), max(data.calendar.end_date_dt)), columns = ['raw_date'])\n",
    "    \n",
    "    # cross join calendar index with actual calendar to get all combos of possible dates & services \n",
    "    calendar_cross = calendar_date_range.merge(data.calendar, how = \"cross\")\n",
    "    \n",
    "    # extract day of week from date index date\n",
    "    calendar_cross['dayofweek'] = calendar_cross['raw_date'].dt.dayofweek\n",
    "    \n",
    "    # take wide calendar data (one col per day of week) and make it long (one row per day of week)\n",
    "    actual_service = calendar_cross.melt(id_vars = ['raw_date', 'start_date_dt', 'end_date_dt', 'start_date', 'end_date', 'service_id', 'dayofweek'], var_name = 'cal_dayofweek', value_name = 'cal_val')\n",
    "    \n",
    "    # map the calendar input strings to day of week integers to align w pandas dayofweek output\n",
    "    actual_service['cal_daynum'] = actual_service['cal_dayofweek'].map({\n",
    "        'monday': 0,\n",
    "        'tuesday': 1,\n",
    "        'wednesday': 2,\n",
    "        'thursday': 3,\n",
    "        'friday': 4,\n",
    "        'saturday': 5,\n",
    "        'sunday': 6\n",
    "    })\n",
    "    \n",
    "    # now check for rows that \"work\"\n",
    "    # i.e., the day of week matches between datetime index & calendar input\n",
    "    # and the datetime index is between the calendar row's start and end dates\n",
    "    actual_service = actual_service[(actual_service.dayofweek == actual_service.cal_daynum) & \n",
    "                                   (actual_service.start_date_dt <= actual_service.raw_date) &\n",
    "                                   (actual_service.end_date_dt >= actual_service.raw_date)]\n",
    "    \n",
    "    # now merge in calendar dates to the datetime index to get overrides\n",
    "    # TODO - localize to Mountain time\n",
    "    actual_service = actual_service.merge(data.calendar_dates, how = 'outer', left_on = ['raw_date', 'service_id'], right_on = ['date_dt', 'service_id'])\n",
    "    \n",
    "    # now add a service happened flag for dates where the schedule indicates that this service occurred\n",
    "    # i.e.: calendar has a service indicator of 1 and there's no exception type from calendar_dates\n",
    "    # OR calendar_dates has exception type of 1\n",
    "    # otherwise no service \n",
    "    # https://stackoverflow.com/questions/21415661/logical-operators-for-boolean-indexing-in-pandas\n",
    "    actual_service['service_happened'] = ((actual_service['cal_val'] == '1') & \n",
    "                                          actual_service['exception_type'].isnull()) | (actual_service['exception_type'] == '1')\n",
    "\n",
    "    \n",
    "    # now fill in rows where calendar_dates had a date outside the bounds of the datetime index, so raw_date is always populated\n",
    "    actual_service['raw_date'] = actual_service['raw_date'].fillna(actual_service['date_dt'])\n",
    "    \n",
    "    # filter to only rows where service occurred\n",
    "    service_happened = actual_service[actual_service.service_happened]\n",
    "    \n",
    "    # join trips to only service that occurred\n",
    "    trips_happened = data.trips.merge(service_happened, how = 'left', on = 'service_id')\n",
    "    \n",
    "    # get only the trip / hour combos that actually occurred\n",
    "    trip_stop_hours = data.stop_times[['trip_id', 'arrival_hour']].drop_duplicates()\n",
    "    \n",
    "    # now join\n",
    "    # result has one row per date + row from trips.txt (incl. route) + hour\n",
    "    trip_summary = trips_happened.merge(trip_stop_hours, how = \"left\", on = \"trip_id\")\n",
    "    \n",
    "    return trip_summary\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fce10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_summary = make_trip_summary(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb947dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trip_summary.dtypes)\n",
    "print(trip_summary.shape)\n",
    "trip_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7ddd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.feed_info.feed_version[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d812de6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION_ID = data.feed_info.feed_version[0]\n",
    "def summarize_and_save(trip_summary): \n",
    "    # now group to get trips by hour by date by route\n",
    "    route_daily_hourly_summary = trip_summary.groupby(by = ['raw_date', 'route_id', 'arrival_hour'])['trip_id'].count().reset_index()\n",
    "\n",
    "    route_daily_hourly_summary.rename(columns = {'arrival_hour': 'hour', 'trip_id': 'trip_count', 'raw_date': 'date'}, inplace = True)\n",
    "    route_daily_hourly_summary.date = route_daily_hourly_summary.date.dt.date\n",
    "    if BUCKET_TYPE == \"private\":\n",
    "        route_daily_hourly_summary.to_csv(f's3://rtd-ghost-buses-{BUCKET_TYPE}/schedule_summaries/route_level/schedule_route_daily_hourly_summary_{VERSION_ID}.csv', index = False)\n",
    "    \n",
    "    # now group to get trips by hour by date by route by *direction*\n",
    "    route_dir_daily_hourly_summary = trip_summary.groupby(by = ['raw_date', 'route_id', 'direction_id', 'arrival_hour'])['trip_id'].count().reset_index()\n",
    "\n",
    "    route_dir_daily_hourly_summary.rename(columns = {'arrival_hour': 'hour', 'trip_id': 'trip_count', 'raw_date': 'date'}, inplace = True)\n",
    "    route_dir_daily_hourly_summary.date = route_dir_daily_hourly_summary.date.dt.date\n",
    "    if BUCKET_TYPE == \"private\":\n",
    "        route_dir_daily_hourly_summary.to_csv(f's3://rtd-ghost-buses-{BUCKET_TYPE}/schedule_summaries/route_dir_level/schedule_route_dir_daily_hourly_summary_{VERSION_ID}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae71821e",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_and_save(trip_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dfd3c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "221fd612",
   "metadata": {},
   "source": [
    "## Most common shape by route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba63340c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get trip count by route, direction, shape id\n",
    "trips_by_rte_direction = data.trips.groupby(['route_id', 'shape_id', 'direction_id'])['trip_id'].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7669b6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only most common shape id by route, direction\n",
    "# follow: https://stackoverflow.com/a/54041328\n",
    "most_common_shapes = trips_by_rte_direction.sort_values('trip_id').drop_duplicates(['route_id','direction_id'],keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb40d8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get additional route attributes\n",
    "most_common_shapes = most_common_shapes.merge(data.routes, how = 'left', on = 'route_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cfd2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make shapely points\n",
    "# https://www.geeksforgeeks.org/apply-function-to-every-row-in-a-pandas-dataframe/\n",
    "data.shapes['pt'] = data.shapes.apply(lambda row: shapely.geometry.Point((float(row['shape_pt_lon']), float(row['shape_pt_lat']))), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77b9d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shapes['shape_pt_sequence'] = pd.to_numeric(data.shapes['shape_pt_sequence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9571f5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct sorted list of shapely points\n",
    "# custom aggregation function: https://stackoverflow.com/a/10964938\n",
    "\n",
    "def make_linestring_of_points(sub_df):\n",
    "    sorted_df = sub_df.sort_values(by = 'shape_pt_sequence')\n",
    "    return shapely.geometry.LineString(list(sorted_df['pt']))\n",
    "\n",
    "constructed_shapes = data.shapes.groupby('shape_id').apply(make_linestring_of_points).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3672e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge in the other route attributes\n",
    "final = most_common_shapes.merge(constructed_shapes, how = 'left', on = 'shape_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdce59ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a \"geometry\" column for geopandas\n",
    "final['geometry'] = final[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd8fd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the geopandas geodataframe\n",
    "final_gdf = geopandas.GeoDataFrame(data = final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba0db89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the column that's a list of shapely points\n",
    "final_gdf = final_gdf.drop(0, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a09727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gis.stackexchange.com/questions/11910/meaning-of-simplifys-tolerance-parameter\n",
    "final_gdf['geometry'] = final_gdf['geometry'].simplify(.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbd55a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file as geojson (this saves locally)\n",
    "with open('route_shapes_simplified_linestring.geojson', 'w') as f:\n",
    "    f.write(final_gdf.loc[(final_gdf['route_type'] == '3')].to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0e4fc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dae4f815",
   "metadata": {},
   "source": [
    "# Exploratory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bfd385d2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3ec208",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.stop_times.loc[data.stop_times['trip_id']=='114354115']\n",
    "# data.calendar\n",
    "# find trip_id's where stop_id = 34327 and stop_sequence = 1\n",
    "data.stop_times.loc[(data.stop_times['stop_id']=='34327') & (data.stop_times['stop_sequence']=='1') ]\n",
    "# 526!\n",
    "# departure_time == \"07:01:00\" #?\n",
    "data.stop_times.loc[(data.stop_times['stop_id']=='34327') & (data.stop_times['stop_sequence']=='1') & (data.stop_times['departure_time']==\"07:01:00\" ) ]\n",
    "\n",
    "data.stop_times.loc[data.stop_times['trip_id']=='114450962']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900dfa27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('rtd_data_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "9a451d7a41f216d3b225fc4be1924d6ac6b639fabac24dabdfa0cec3d0a5c1d1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
